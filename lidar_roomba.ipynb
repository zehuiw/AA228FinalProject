{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AA228/CS238 Optional Final Project: Escape Roomba\n",
    "\n",
    "This notebook demonstrates how to interact with the Roomba environment. We show how to:\n",
    "1. Import the necessary packages\n",
    "2. Define the sensor and construct the POMDP\n",
    "3. Set up a particle filter\n",
    "4. Define a policy\n",
    "5. Simulate and render the environment\n",
    "6. Evaluate the policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"/Users/little-orange/Documents/18Aut/238/AA228FinalProject/Project.toml\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# activate project environment\n",
    "# include these lines of code in any future scripts/notebooks\n",
    "#---\n",
    "import Pkg\n",
    "if !haskey(Pkg.installed(), \"AA228FinalProject\")\n",
    "    jenv = joinpath(dirname(@__FILE__()), \".\") # this assumes the notebook is in the same dir\n",
    "    # as the Project.toml file, which should be in top level dir of the project. \n",
    "    # Change accordingly if this is not the case.\n",
    "    Pkg.activate(jenv)\n",
    "end\n",
    "#---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "using AA228FinalProject\n",
    "using POMDPs\n",
    "using POMDPPolicies\n",
    "using BeliefUpdaters\n",
    "using ParticleFilters\n",
    "using POMDPSimulators\n",
    "using Cairo\n",
    "using Gtk\n",
    "using Random\n",
    "using Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sensor and construct POMDP\n",
    "In the following cell, we first instantiate a sensor. There are two sensors implemented: a lidar sensor and a bump sensor. The lidar sensor receives a single noisy measurement of the distance to the nearest wall in the direction the Roomba is facing. The bump sensor indicates when contact has been made between any part of the Roomba and any wall.\n",
    "\n",
    "Next, we instantiate the MDP, which defines the underlying simulation environment, assuming full observability. The MDP takes many arguments to specify details of the problem. Feel free to examine the underlying source code (```src/roomba_env.jl```) to gain an understanding for these arguments. One argument we must specify here is the ```config```. This argument, which can take values 1,2, or 3, specifies the room configuration, with each configuration corresponding to a different location for the goal and stairs.\n",
    "\n",
    "Finally, we instantiate the POMDP. The POMDP takes as an argument the underlying MDP as well as the sensor, which it uses to define the observation model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121-element Array{RoombaAct,1}:\n",
       " [0.0, -1.0] \n",
       " [1.0, -1.0] \n",
       " [2.0, -1.0] \n",
       " [3.0, -1.0] \n",
       " [4.0, -1.0] \n",
       " [5.0, -1.0] \n",
       " [6.0, -1.0] \n",
       " [7.0, -1.0] \n",
       " [8.0, -1.0] \n",
       " [9.0, -1.0] \n",
       " [10.0, -1.0]\n",
       " [0.0, -0.8] \n",
       " [1.0, -0.8] \n",
       " ⋮           \n",
       " [10.0, 0.8] \n",
       " [0.0, 1.0]  \n",
       " [1.0, 1.0]  \n",
       " [2.0, 1.0]  \n",
       " [3.0, 1.0]  \n",
       " [4.0, 1.0]  \n",
       " [5.0, 1.0]  \n",
       " [6.0, 1.0]  \n",
       " [7.0, 1.0]  \n",
       " [8.0, 1.0]  \n",
       " [9.0, 1.0]  \n",
       " [10.0, 1.0] "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_x_pts = 50\n",
    "num_y_pts = 50\n",
    "num_th_pts = 20\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "\n",
    "vlist = [i for i in 0:10]\n",
    "omlist = [i for i in -1:0.2:1]\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in vlist, om in omlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor = Bumper() # or Bumper() for the bumper version of the environment\n",
    "config = 1 # 1,2, or 3\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, sspace=sspace, aspace=aspace));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Particle Filter\n",
    "\n",
    "To solve a POMDP, we must specify a method for representing the belief state and updating it given an observation and an action. Here, we demonstrate how to instantiate a particle filter, which has been implemented in the package ```ParticleFilters.jl```.\n",
    "\n",
    "First, we instantiate a resampler, which is responsible for updating the belief state given an observation. We have implemented a resampler for each of the lidar and bumper environments, the source code for which can be found in ```src/filtering.jl```. The first argument for both resamplers is the number of particles that represent the belief state. The lidar resampler takes a low-variance resampler as an additional argument, implemented for us in ```ParticleFilters.jl```, which is responsible for efficiently resampling a weighted set of particles. \n",
    "\n",
    "Next, we instantiate a ```SimpleParticleFilter```, which enables us to perform our belief updates, and is implemented for us in ```ParticleFilters.jl```, .\n",
    "\n",
    "Finally, we pass this particle filter into a custom struct called a ```RoombaParticleFilter```, which takes two additional arguments. These arguments specify the noise in the velocity and turn-rate, used when propegating particles according to the action taken. These can be tuned depending on the type of sensor used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_particles = 2000\n",
    "# resampler = LidarResampler(num_particles, LowVarianceResampler(num_particles))\n",
    "# for the bumper environment\n",
    "resampler = BumperResampler(num_particles)\n",
    "\n",
    "spf = SimpleParticleFilter(m, resampler)\n",
    "\n",
    "v_noise_coefficient = 2.0\n",
    "om_noise_coefficient = 0.5\n",
    "\n",
    "belief_updater = RoombaParticleFilter(spf, v_noise_coefficient, om_noise_coefficient);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a policy\n",
    "\n",
    "Here we demonstrate how to define a naive policy that attempts navigate the Roomba to the goal. The heuristic policy we define here first spins around for 25 time-steps in order to perform localization, then follows a simple proprtional control law that navigates the robot in the direction of the goal state (note that this policy fails if there is a wall in the way).\n",
    "\n",
    "First we create a struct that subtypes the Policy abstract type, defined in the package ```POMDPPolicies.jl```. Here, we can also define certain parameters, such as a variable tracking the current time-step.\n",
    "\n",
    "Next, we define a function that can take in our policy and the belief state and return the desired action. We do this by defining a new ```POMDPs.action``` function that will work with our policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QMDPSolver"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutable struct QMDPSolver <: Solver\n",
    "    max_iteration::Int64\n",
    "    tolerance::Float64\n",
    "end\n",
    "QMDPSolver(;max_iterations::Int64=100, tolerance::Float64=1e-3) = QMDPSolver(max_iterations, tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QMDPPolicy"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using POMDPModelTools # for ordered_actions\n",
    "mutable struct QMDPPolicy <: Policy\n",
    "    alphas::Matrix{Float64} # matrix of alpha vectors |S|x|A|\n",
    "    action_map::Vector{Any} # maps indices to actions\n",
    "    pomdp::POMDP            # models for convenience\n",
    "end\n",
    "# default constructor\n",
    "function QMDPPolicy(pomdp::POMDP)\n",
    "    ns = n_states(pomdp)\n",
    "    na = n_actions(pomdp)\n",
    "    alphas = zeros(ns, na)\n",
    "    am = Any[]\n",
    "    space = ordered_actions(pomdp)\n",
    "#     println(\"action_space:\", space)\n",
    "#     for a in iterator(space)\n",
    "    for a in space\n",
    "        push!(am, a)\n",
    "    end\n",
    "    return QMDPPolicy(alphas, am, pomdp)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.solve(solver::QMDPSolver, pomdp::POMDP)\n",
    "\n",
    "    policy = QMDPPolicy(pomdp)\n",
    "\n",
    "    # get solver parameters\n",
    "    max_iterations = solver.max_iteration\n",
    "    tolerance = solver.tolerance\n",
    "    discount_factor = discount(pomdp)\n",
    "\n",
    "    # intialize the alpha-vectors\n",
    "    alphas = policy.alphas\n",
    "\n",
    "    # initalize space\n",
    "    sspace = ordered_states(pomdp)  # returns a discrete state space object of the pomdp\n",
    "    aspace = ordered_actions(pomdp) # returns a discrete action space object\n",
    "\n",
    "    # main loop\n",
    "    for i = 1:max_iterations\n",
    "        residual = 0.0\n",
    "        # state loop\n",
    "        for (istate, s) in enumerate(sspace)\n",
    "            old_alpha = maximum(alphas[istate,:]) # for residual\n",
    "            max_alpha = -Inf\n",
    "            # action loop\n",
    "            # alpha(s) = R(s,a) + discount_factor * sum(T(s'|s,a)max(alpha(s'))\n",
    "            for (iaction, a) in enumerate(aspace)\n",
    "                # the transition function modifies the dist argument to a distribution availible from that state-action pair\n",
    "                dist = transition(pomdp, s, a) # fills distribution over neighbors\n",
    "                q_new = 0.0\n",
    "#                 for sp in iterator(dist)\n",
    "#                 println(\"s:\", s)\n",
    "#                 println(\"a:\", a)\n",
    "#                 println(\"dist:\", dist)\n",
    "#                 println(\"type of dist: \", typeof(dist))\n",
    "                \n",
    "#                 for sp in dist\n",
    "                    # pdf returns the probability mass of sp in dist\n",
    "                sp = dist\n",
    "                p = pdf(dist, sp)\n",
    "                p == 0.0 ? continue : nothing # skip if zero prob\n",
    "                # returns the reward from s-a-sp triple\n",
    "                r = reward(pomdp, s, a, sp)\n",
    "\n",
    "                # stateindex returns an integer\n",
    "                sidx = stateindex(pomdp, sp)\n",
    "                q_new += p * (r + discount_factor * maximum(alphas[sidx,:]))\n",
    "#                 end\n",
    "                \n",
    "                \n",
    "                new_alpha = q_new\n",
    "                alphas[istate, iaction] = new_alpha\n",
    "                new_alpha > max_alpha ? (max_alpha = new_alpha) : nothing\n",
    "            end # actiom\n",
    "            # update the value array\n",
    "            diff = abs(max_alpha - old_alpha)\n",
    "            diff > residual ? (residual = diff) : nothing\n",
    "        end # state\n",
    "        # check if below Bellman residual\n",
    "        residual < tolerance ? break : nothing\n",
    "    end # main\n",
    "    # return the policy\n",
    "    policy\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "function POMDPs.action(policy::QMDPPolicy, b::DiscreteBelief)\n",
    "    alphas = policy.alphas\n",
    "    ihi = 0\n",
    "    vhi = -Inf\n",
    "    (ns, na) = size(alphas)\n",
    "    @assert length(b.b) == ns \"Length of belief and alpha-vector size mismatch\"\n",
    "    # see which action gives the highest util value\n",
    "    for ai = 1:na\n",
    "        util = dot(alphas[:,ai], b.b)\n",
    "        if util > vhi\n",
    "            vhi = util\n",
    "            ihi = ai\n",
    "        end\n",
    "    end\n",
    "    # map the index to action\n",
    "    return policy.action_map[ihi]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the policy to test\n",
    "# mutable struct ToEnd <: Policy\n",
    "#     ts::Int64 # to track the current time-step.\n",
    "# end\n",
    "\n",
    "# # extract goal for heuristic controller\n",
    "# goal_xy = get_goal_xy(m)\n",
    "\n",
    "# # define a new function that takes in the policy struct and current belief,\n",
    "# # and returns the desired action\n",
    "# function POMDPs.action(p::ToEnd, b::ParticleCollection{RoombaState})\n",
    "    \n",
    "#     # spin around to localize for the first 25 time-steps\n",
    "#     if p.ts < 25\n",
    "#         p.ts += 1\n",
    "#         return RoombaAct(0.,1.0) # all actions are of type RoombaAct(v,om)\n",
    "#     end\n",
    "#     p.ts += 1\n",
    "    \n",
    "#     # after 25 time-steps, we follow a proportional controller to navigate\n",
    "#     # directly to the goal, using the mean belief state\n",
    "    \n",
    "#     # compute mean belief of a subset of particles\n",
    "#     s = mean(b)\n",
    "    \n",
    "#     # compute the difference between our current heading and one that would\n",
    "#     # point to the goal\n",
    "#     goal_x, goal_y = goal_xy\n",
    "#     x,y,th = s[1:3]\n",
    "#     ang_to_goal = atan(goal_y - y, goal_x - x)\n",
    "#     del_angle = wrap_to_pi(ang_to_goal - th)\n",
    "    \n",
    "#     # apply proportional control to compute the turn-rate\n",
    "#     Kprop = 1.0\n",
    "#     om = Kprop * del_angle\n",
    "    \n",
    "#     # always travel at some fixed velocity\n",
    "#     v = 5.0\n",
    "    \n",
    "#     return RoombaAct(v, om)\n",
    "# end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation and rendering\n",
    "\n",
    "Here, we will demonstrate how to seed the environment, run a simulation, and render the simulation. To render the simulation, we use the ```Gtk``` package. \n",
    "\n",
    "The simulation is carried out using the ```stepthrough``` function defined in the package ```POMDPSimulators.jl```. During a simulation, a window will open that renders the scene. It may be hidden behind other windows on your desktop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QMDPPolicy([0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; … ; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0], Any[[0.0, -1.0], [1.0, -1.0], [2.0, -1.0], [3.0, -1.0], [4.0, -1.0], [5.0, -1.0], [6.0, -1.0], [7.0, -1.0], [8.0, -1.0], [9.0, -1.0]  …  [1.0, 1.0], [2.0, 1.0], [3.0, 1.0], [4.0, 1.0], [5.0, 1.0], [6.0, 1.0], [7.0, 1.0], [8.0, 1.0], [9.0, 1.0], [10.0, 1.0]], RoombaPOMDP{Bumper,Bool}(Bumper(), RoombaMDP{DiscreteRoombaStateSpace,Array{RoombaAct,1}}\n",
       "  v_max: Float64 10.0\n",
       "  om_max: Float64 1.0\n",
       "  dt: Float64 0.5\n",
       "  contact_pen: Float64 -1.0\n",
       "  time_pen: Float64 -0.1\n",
       "  goal_reward: Float64 10.0\n",
       "  stairs_penalty: Float64 -10.0\n",
       "  config: Int64 1\n",
       "  room: AA228FinalProject.Room\n",
       "  sspace: DiscreteRoombaStateSpace\n",
       "  aspace: Array{RoombaAct}((121,))\n",
       "  _amap: Dict{RoombaAct,Int64}\n",
       "))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first seed the environment\n",
    "Random.seed!(2)\n",
    "\n",
    "# reset the policy\n",
    "# p = ToEnd(0) # here, the argument sets the time-steps elapsed to 0\n",
    "\n",
    "solver = QMDPSolver(1, 1e-3)\n",
    "policy = solve(solver, m)\n",
    "\n",
    "# # run the simulation\n",
    "# c = @GtkCanvas()\n",
    "# win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "# for (t, step) in enumerate(stepthrough(m, policy, belief_updater, max_steps=100))\n",
    "#     @guarded draw(c) do widget\n",
    "        \n",
    "#         # the following lines render the room, the particles, and the roomba\n",
    "#         ctx = getgc(c)\n",
    "#         set_source_rgb(ctx,1,1,1)\n",
    "#         paint(ctx)\n",
    "#         render(ctx, m, step)\n",
    "        \n",
    "#         # render some information that can help with debugging\n",
    "#         # here, we render the time-step, the state, and the observation\n",
    "#         move_to(ctx,300,400)\n",
    "#         show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "#     end\n",
    "#     show(c)\n",
    "#     sleep(0.1) # to slow down the simulation\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: mdp not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: mdp not defined",
      "",
      "Stacktrace:",
      " [1] initialstate(::RoombaPOMDP{Bumper,Bool}, ::MersenneTwister) at ./In[85]:2",
      " [2] rand(::MersenneTwister, ::AA228FinalProject.RoombaInitialDistribution{RoombaPOMDP{Bumper,Bool}}) at /Users/little-orange/Documents/18Aut/238/AA228FinalProject/src/roomba_env.jl:420",
      " [3] get_initialstate(::StepSimulator, ::AA228FinalProject.RoombaInitialDistribution{RoombaPOMDP{Bumper,Bool}}) at /Users/little-orange/.julia/packages/POMDPSimulators/xyfJM/src/history_recorder.jl:256",
      " [4] simulate(::StepSimulator, ::RoombaPOMDP{Bumper,Bool}, ::QMDPPolicy, ::RoombaParticleFilter, ::AA228FinalProject.RoombaInitialDistribution{RoombaPOMDP{Bumper,Bool}}, ::Nothing) at /Users/little-orange/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:31",
      " [5] simulate at /Users/little-orange/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:30 [inlined]",
      " [6] simulate(::StepSimulator, ::RoombaPOMDP{Bumper,Bool}, ::QMDPPolicy, ::RoombaParticleFilter) at /Users/little-orange/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:26",
      " [7] #stepthrough#19(::Base.Iterators.Pairs{Symbol,Int64,Tuple{Symbol},NamedTuple{(:max_steps,),Tuple{Int64}}}, ::Function, ::RoombaPOMDP{Bumper,Bool}, ::QMDPPolicy, ::RoombaParticleFilter) at /Users/little-orange/.julia/packages/POMDPSimulators/xyfJM/src/stepthrough.jl:222",
      " [8] (::getfield(POMDPSimulators, Symbol(\"#kw##stepthrough\")))(::NamedTuple{(:max_steps,),Tuple{Int64}}, ::typeof(stepthrough), ::RoombaPOMDP{Bumper,Bool}, ::QMDPPolicy, ::RoombaParticleFilter) at ./none:0",
      " [9] top-level scope at In[88]:4"
     ]
    }
   ],
   "source": [
    "# run the simulation\n",
    "c = @GtkCanvas()\n",
    "win = GtkWindow(c, \"Roomba Environment\", 600, 600)\n",
    "for (t, step) in enumerate(stepthrough(m, policy, belief_updater, max_steps=100))\n",
    "    @guarded draw(c) do widget\n",
    "        \n",
    "        # the following lines render the room, the particles, and the roomba\n",
    "        ctx = getgc(c)\n",
    "        set_source_rgb(ctx,1,1,1)\n",
    "        paint(ctx)\n",
    "        render(ctx, m, step)\n",
    "        \n",
    "        # render some information that can help with debugging\n",
    "        # here, we render the time-step, the state, and the observation\n",
    "        move_to(ctx,300,400)\n",
    "        show_text(ctx, @sprintf(\"t=%d, state=%s, o=%.3f\",t,string(step.s),step.o))\n",
    "    end\n",
    "    show(c)\n",
    "    sleep(0.1) # to slow down the simulation\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifying initial states and beliefs\n",
    "If, for debugging purposes, you would like to hard-code a starting location or initial belief state for the roomba, you can do so by taking the following steps.\n",
    "\n",
    "First, we define the initial state using the following line of code:\n",
    "```\n",
    "is = RoombaState(x,y,th,0.)\n",
    "```\n",
    "Where ```x``` and ```y``` are the x,y coordinates of the starting location and ```th``` is the heading in radians. The last entry, ```0.```, respresents whether the state is terminal, and should remain unchanged.\n",
    "\n",
    "If you would like to initialize the Roomba's belief as perfectly localized, you can do so with the following line of code:\n",
    "```\n",
    "b0 = Deterministic(is)\n",
    "```\n",
    "If you would like to initialize the standard unlocalized belief, use these lines:\n",
    "```\n",
    "dist = initialstate_distribution(m)\n",
    "b0 = initialize_belief(belief_updater, dist)\n",
    "```\n",
    "Finally, we call the ```stepthrough``` function using the initial state and belief as follows:\n",
    "```\n",
    "stepthrough(m,planner,belief_updater,b0,is,max_steps=300)\n",
    "```\n",
    "\n",
    "### Discretizing the state space\n",
    "Certain POMDP solution techniques require discretizing the state space. Should we need to do so, we first define the state space by specifying the number of points to discretize the range of possible x, y, and $\\theta$ values, and then calling the DiscreteRoombaStateSpace constructor.\n",
    "```\n",
    "num_x_pts = ... # e.g. 50\n",
    "num_y_pts = ... # e.g. 50\n",
    "num_th_pts = ... # e.g. 20\n",
    "sspace = DiscreteRoombaStateSpace(num_x_pts,num_y_pts,num_th_pts)\n",
    "```\n",
    "\n",
    "Next, we pass in the state space as an argument when constructing the POMDP.\n",
    "```\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, sspace=sspace))\n",
    "```\n",
    "\n",
    "### Discretizing the action space\n",
    "Certain POMDP solution techniques require discretizing the action space. Should we need to do so, we first define the action space as follows:\n",
    "```\n",
    "vlist = [...]\n",
    "omlist = [...]\n",
    "aspace = vec(collect(RoombaAct(v, om) for v in vlist, om in omlist))\n",
    "```\n",
    "Where ```vlist``` is an array of possible values for the velocity (e.g. ```[0,1,10]```) and ```omlist``` is an array of possible turn-rates (e.g. ```[-1,0,1]```).\n",
    "\n",
    "Next, we pass in the action space as an argument when constructing the POMDP.\n",
    "```\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config, aspace=aspace))\n",
    "```\n",
    "\n",
    "### Discretizing the Lidar observation space\n",
    "Certain POMDP solution techniques require discretizing the observation space. The Bumper sensor has a discrete observation space by default, while the Lidar sensor is continuous by default. The observations can take values in the range $[0,\\infty)$. Should we need to do discretize the Lidar observation space, we first define cut-points $x_{1:K}$ such that all observations between $-\\infty$ and $x_1$ are considered the discrete observation 1, all observations between $x_1$ and $x_2$ are considered discrete observation 2, and so on. All observations between $x_K$ and $\\infty$ are considered discrete observation $K+1$. We instantiate the discretized sensor as follows:\n",
    "```\n",
    "cut_points = [x_1, x_2, ..., x_K]\n",
    "sensor = DiscreteLidar(cut_points)\n",
    "```\n",
    "Next, we pass in the sensor as an argument when constructing the POMDP as usual.\n",
    "```\n",
    "m = RoombaPOMDP(sensor=sensor, mdp=RoombaMDP(config=config))\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "\n",
    "Here, we demonstate a simple evaluation of the policy's performance for a few random seeds. This is meant to serve only as an example, and we encourage you to develop your own evaluation metrics.\n",
    "\n",
    "We intialize the robot using five different random seeds, and simulate its performance for 100 time-steps. We then sum the rewards experienced during its interaction with the environment and track this total reward for the five trials.\n",
    "Finally, we report the mean and standard error for the total reward. The standard error is the standard deviation of a sample set divided by the square root of the number of samples, and represents the uncertainty in the estimate of the mean value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Mean Total Reward: -1.760, StdErr Total Reward: 3.980"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "total_rewards = []\n",
    "\n",
    "for exp = 1:5\n",
    "    println(string(exp))\n",
    "    \n",
    "    Random.seed!(exp)\n",
    "    \n",
    "    p = ToEnd(0)\n",
    "    traj_rewards = sum([step.r for step in stepthrough(m,p,belief_updater, max_steps=100)])\n",
    "    \n",
    "    push!(total_rewards, traj_rewards)\n",
    "end\n",
    "\n",
    "@printf(\"Mean Total Reward: %.3f, StdErr Total Reward: %.3f\", mean(total_rewards), std(total_rewards)/sqrt(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.1",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
